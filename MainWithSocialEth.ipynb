{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params\n",
    "kld_reg= 1\n",
    "adl_reg=1\n",
    "\n",
    "fdim=16\n",
    "zdim=16\n",
    "sigma=1.3\n",
    "past_length=8\n",
    "future_length=12\n",
    "data_scale=1.86\n",
    "\n",
    "enc_past_size=(past_length*2,512,256,fdim)\n",
    "enc_dest_size=(2,8,16,fdim)\n",
    "\n",
    "enc_latent_size=(2*fdim,8,50,2*zdim)\n",
    "dec_size=(fdim + zdim,1024,512,1024,2)\n",
    "\n",
    "predictor_size=(2*fdim + 2,1024,512,256,2*(future_length-1))\n",
    "\n",
    "non_local_theta_size = (2*fdim + 2,256,128,64,128)\n",
    "non_local_phi_size=(2*fdim + 2,256,128,64,128)\n",
    "non_local_g_size=(2*fdim + 2,256,128,64,2*fdim + 2)\n",
    "nonlocal_pools=3\n",
    "learning_rate=0.0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(file_path: str):\n",
    "  npz = np.load(file_path, allow_pickle=True)\n",
    "  return npz['observations'], npz['obs_speed'], npz['targets'], npz[\n",
    "      'target_speed'], npz['mean'], npz['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(tf.Module):\n",
    "  def __init__(self, input_dim, output_size, name=None):\n",
    "    super(Dense, self).__init__(name=name)\n",
    "    self.w = tf.Variable(tf.random.uniform([input_dim, output_size],-(1.0/input_dim)**0.5,(1.0/input_dim)**0.5 ),name='w',dtype=tf.float32,trainable=True)\n",
    "    self.b = tf.Variable(tf.random.uniform([output_size],-(1.0/input_dim)**0.5,(1.0/input_dim)**0.5 ), name='b',dtype=tf.float32,trainable=True)\n",
    "  def __call__(self, x):\n",
    "    #x = tf.constant(x,dtype=tf.float32)\n",
    "    y = tf.matmul(x, self.w) + self.b\n",
    "    return y\n",
    "\n",
    "class FullyConnectedNeuralNet(tf.Module):\n",
    "  def __init__(self,sizes, name=None):\n",
    "    super(FullyConnectedNeuralNet, self).__init__(name=name)\n",
    "    self.layers = []\n",
    "    with self.name_scope:\n",
    "      for i in range(len(sizes)-1):\n",
    "        self.layers.append(Dense(input_dim=sizes[i], output_size=sizes[i+1]))\n",
    "  @tf.Module.with_name_scope\n",
    "  def __call__(self, x):\n",
    "    for i,layer in enumerate(self.layers):\n",
    "        x=layer(x)\n",
    "        if i != len(self.layers)-1:\n",
    "            x = tf.nn.relu(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainModel(tf.Module):\n",
    "    def __init__(self,name=None):\n",
    "        super(MainModel, self).__init__(name=name)\n",
    "\n",
    "        self.zdim = zdim\n",
    "        self.sigma = sigma\n",
    "        self.nonlocal_pools = nonlocal_pools\n",
    "\n",
    "        self.pastEncoder = FullyConnectedNeuralNet(enc_past_size)\n",
    "\n",
    "        self.destEncoder = FullyConnectedNeuralNet(enc_dest_size)\n",
    "\n",
    "        self.latentDistributionEncoder = FullyConnectedNeuralNet(enc_latent_size)\n",
    "\n",
    "        self.latentDistributionDecoder = FullyConnectedNeuralNet(dec_size)\n",
    "\n",
    "    \n",
    "        self.nonLocalTheta = FullyConnectedNeuralNet(non_local_theta_size)\n",
    "        self.nonLocalPhi = FullyConnectedNeuralNet(non_local_phi_size)\n",
    "        self.nonLocalG = FullyConnectedNeuralNet(non_local_g_size)\n",
    "        \n",
    "        self.predictorNetwork = FullyConnectedNeuralNet(predictor_size)\n",
    "    \n",
    "    def forward(self, x, initial_pos, dest=[], mask =[] ):\n",
    "\n",
    "        if len(dest):\n",
    "            self.training=True\n",
    "        else:\n",
    "            self.training=False\n",
    "        \n",
    "        # encode\n",
    "        traj_past_ftr = self.pastEncoder(x)\n",
    "        #print(f\"ftraj max {ftraj.numpy().max()}\")\n",
    "        if not self.training:\n",
    "            z = tf.random.normal((x.shape[0], self.zdim),0,self.sigma)\n",
    "\n",
    "        else:\n",
    "            dest_ftr = self.destEncoder(dest)\n",
    "            #print(f\"dest_features Max {dest_features.numpy().max()}\")\n",
    "\n",
    "            concat_ftr = tf.concat((traj_past_ftr, dest_ftr), axis = 1)\n",
    "            latent =  self.latentDistributionEncoder(concat_ftr)\n",
    "            mu = latent[:, 0:self.zdim] # 2-d array\n",
    "            logvar = latent[:, self.zdim:] # 2-d array\n",
    "\n",
    "            var = tf.math.exp(logvar*0.5)\n",
    "            #print(f\"var {var}\")\n",
    "            eps = tf.random.normal(var.shape)\n",
    "            #z = tf.Variable(eps*var + mu,dtype=tf.float32)\n",
    "            z = eps*var + mu\n",
    "            \n",
    "            #print(f\"z -> {z}\")\n",
    "\n",
    "\n",
    "        latentDistributionDecoder_input = tf.concat((traj_past_ftr, z), axis = 1)\n",
    "        generated_dest = self.latentDistributionDecoder(latentDistributionDecoder_input)\n",
    "        #generated_dest = tf.where(tf.math.is_nan(generated_dest), tf.zeros_like(generated_dest), generated_dest)\n",
    "\n",
    "        if self.training:\n",
    "            generated_dest_ftr = self.destEncoder(generated_dest)\n",
    "            #generated_dest_ftr = tf.where(tf.math.is_nan(generated_dest_ftr), tf.zeros_like(generated_dest_ftr), generated_dest_ftr)\n",
    "            #print(f\"{ tf.math.reduce_any(tf.math.is_nan(generated_dest_ftr))}\")\n",
    "\n",
    "            prediction_ftr = tf.concat((traj_past_ftr, generated_dest_ftr,initial_pos), axis = 1)\n",
    "            for i in range(self.nonlocal_pools):\n",
    "                prediction_ftr = self.nonLocalSocialPooling(prediction_ftr, mask)\n",
    "            pred_future = self.predictorNetwork(prediction_ftr)\n",
    "        \n",
    "            return generated_dest, mu, logvar, pred_future\n",
    "        else:\n",
    "            return generated_dest\n",
    "    \n",
    "    def nonLocalSocialPooling(self, feat, mask):\n",
    "        # N,C\n",
    "        theta_x = self.nonLocalTheta(feat)\n",
    "        # C,N\n",
    "        phi_x = tf.transpose(self.nonLocalPhi(feat))\n",
    "\n",
    "        # f_ij = (theta_i)^T(phi_j), (N,N)\n",
    "        f = tf.matmul(theta_x , phi_x)\n",
    "\n",
    "        # f_weights_i =  exp(f_ij)/(\\sum_{j=1}^N exp(f_ij))\n",
    "        f_weights = tf.nn.softmax(f, axis = -1)\n",
    "        # setting weights of non neighbours to zero\n",
    "        f_weights = f_weights * mask\n",
    "        pooled_g = self.nonLocalG(feat)\n",
    "        \n",
    "        #print(f\"f_weights {f_weights.shape}\")\n",
    "        #print(f\"self.pooled_g(feat) {pooled_g.shape}\")\n",
    "        \n",
    "        # rescaling row weights to 1\n",
    "        f_weights = tf.math.l2_normalize(f_weights,axis=1)\n",
    "        #print(f\"f_weights {f_weights.shape}\")\n",
    "\n",
    "        # ith row of all_pooled_f = \\sum_{j=1}^N f_weights_i_j * g_row_j\n",
    "        pooled_f = tf.matmul(f_weights, pooled_g)\n",
    "\n",
    "        return pooled_f + feat\n",
    "\n",
    "    def predict(self, past, generated_dest, mask, initial_pos):\n",
    "        \n",
    "        traj_past_ftr = self.pastEncoder(past)\n",
    "        generated_dest_ftr = self.destEncoder(generated_dest)\n",
    "        prediction_ftr = tf.concat((traj_past_ftr, generated_dest_ftr,initial_pos), axis = 1)\n",
    "        for i in range(self.nonlocal_pools):\n",
    "            prediction_ftr = self.nonLocalSocialPooling(prediction_ftr, mask)   \n",
    "        future_traj = self.predictorNetwork(prediction_ftr)\n",
    "        return future_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(dest, dest_rec, mean, log_var, future, future_rec):\n",
    "    \n",
    "    rcl = tf.math.reduce_mean(tf.keras.metrics.mean_squared_error(dest, dest_rec))\n",
    "    adl = tf.math.reduce_mean(tf.keras.metrics.mean_squared_error(future, future_rec))\n",
    "\n",
    "    kld = -0.5 * tf.math.reduce_sum(1 + log_var - mean**2 - tf.math.exp(log_var))\n",
    "\n",
    "    return rcl, kld, adl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(X,batchSize):\n",
    "    start = random.randint(0, len(X)-batchSize)\n",
    "    return X[start:start+batchSize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer):\n",
    "    #trajectory_batches,mask_batches,initial_pos_batches = loadDataSocial('./social_pool_data/train_all_512_0_100.pickle',set_name=\"train\")\n",
    "    trajectory_batches,mask_batches,initial_pos_batches = loadDataSocial('./SocialData/social_zara2_train_256_0_50.pickle',\"train\")\n",
    "    train_loss = 0\n",
    "    total_rcl, total_kld, total_adl = 0, 0, 0\n",
    "    \n",
    "    for i, (traj, mask, initial_pos) in enumerate(zip(trajectory_batches,mask_batches,initial_pos_batches)):\n",
    "        traj -= traj[:, :1, :]\n",
    "        traj *= data_scale\n",
    "        x = traj[:, :past_length, :]\n",
    "        y = traj[:, past_length:, :]\n",
    "\n",
    "        x = x.reshape(-1, x.shape[1]*x.shape[2]) # (x,y,x,y ... )\n",
    "        dest = y[:, -1, :]\n",
    "        future = y[:, :-1, :].reshape(y.shape[0],-1)\n",
    "        #x.astype(np.float64)\n",
    "        #print(f\"trajx-> {trajx.shape}\")\n",
    "        \n",
    "        #print(f\"X shape -> {x.shape}\")\n",
    "        #print(f\"initial_pos -> {initial_pos.shape}\")\n",
    "        #print(f\"dest shape -> {dest.shape}\")\n",
    "        #print(f\"mask shape -> {mask.shape}\")\n",
    "        x=np.float32(x)\n",
    "        dest=np.float32(dest)\n",
    "        initial_pos=np.float32(initial_pos)\n",
    "        mask=np.float32(mask)\n",
    "        with tf.GradientTape() as tape:\n",
    "            #x=tf.constant(x,dtype=tf.float32)\n",
    "            #initial_pos=tf.constant(initial_pos,dtype=tf.float32)\n",
    "            #dest=tf.constant(dest,dtype=tf.float32)\n",
    "            dest_rec, mu, var, future_rec = model.forward(x, initial_pos, dest=dest, mask=mask)\n",
    "            #print(f\"dest_recon {dest_recon}\")\n",
    "            #print(f\"mu {mu}\")\n",
    "            #print(f\"var {var}\")\n",
    "            #print(f\"interpolated_future {interpolated_future}\") \n",
    "            #print([var.name for var in tape.watched_variables()])\n",
    "            rcl, kld, adl = calculate_loss(dest, dest_rec, mu, var, future, future_rec)   \n",
    "            loss = rcl + kld * kld_reg + adl * adl_reg\n",
    "            #print(f\"loss -> {loss}\")\n",
    "        \n",
    "        grad_sub = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grad_sub, model.trainable_variables))\n",
    "        \"\"\"\"\n",
    "        with tf.GradientTape() as tape1,tf.GradientTape() as tape2,tf.GradientTape() as tape3,tf.GradientTape() as tape4,tf.GradientTape() as tape5,tf.GradientTape() as tape6,tf.GradientTape() as tape7,tf.GradientTape() as tape8:      \n",
    "            #x=tf.constant(x,dtype=tf.float32)\n",
    "            #initial_pos=tf.constant(initial_pos,dtype=tf.float32)\n",
    "            #dest=tf.constant(dest,dtype=tf.float32)\n",
    "            dest_rec, mu, var, future_rec = model.forward(x, initial_pos, dest=dest, mask=mask)\n",
    "            #print(f\"dest_recon {dest_recon}\")\n",
    "            #print(f\"mu {mu}\")\n",
    "            #print(f\"var {var}\")\n",
    "            #print(f\"interpolated_future {interpolated_future}\") \n",
    "            #print([var.name for var in tape.watched_variables()])\n",
    "            rcl, kld, adl = calculate_loss(dest, dest_rec, mu, var, future, future_rec)   \n",
    "            #loss = rcl + kld * kld_reg + adl * adl_reg\n",
    "            #print(f\"loss -> {loss}\")\n",
    "        \n",
    "            \n",
    "            loss=(rcl+kld)\n",
    "        grad_sub1 = tape1.gradient(adl, model.pastEncoder.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grad_sub1, model.pastEncoder.trainable_variables))\n",
    "        grad_sub2 = tape2.gradient(rcl, model.destEncoder.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grad_sub2, model.destEncoder.trainable_variables))\n",
    "        grad_sub3 = tape3.gradient(loss, model.latentDistributionEncoder.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grad_sub3, model.latentDistributionEncoder.trainable_variables))\n",
    "        grad_sub4 = tape4.gradient(loss, model.latentDistributionDecoder.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grad_sub4, model.latentDistributionDecoder.trainable_variables))\n",
    "        grad_sub5 = tape5.gradient(adl, model.nonLocalTheta.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grad_sub5, model.nonLocalTheta.trainable_variables))\n",
    "        grad_sub6 = tape6.gradient(adl, model.nonLocalPhi.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grad_sub6, model.nonLocalPhi.trainable_variables))\n",
    "        grad_sub7 = tape7.gradient(adl, model.nonLocalG.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grad_sub7, model.nonLocalG.trainable_variables))\n",
    "        grad_sub8 = tape8.gradient(adl, model.predictorNetwork.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grad_sub8, model.predictorNetwork.trainable_variables))\n",
    "        \"\"\"\n",
    "                    \n",
    "        #print(f\"total Loss {loss}\")\n",
    "        #print(f\"rcl Loss {rcl}\")\n",
    "        #print(f\"kld Loss {kld}\")\n",
    "        #print(f\"adl Loss {adl}\")\n",
    "        train_loss+=loss\n",
    "        total_rcl+=rcl\n",
    "        total_kld+=kld\n",
    "        total_adl+=adl\n",
    "    return train_loss, total_rcl, total_kld, total_adl\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, best_of_n = 1):\n",
    "    #trajectory_batches,mask_batches,initial_pos_batches = loadDataSocial('./social_pool_data/test_all_4096_0_100.pickle',set_name=\"test\")\n",
    "    trajectory_batches,mask_batches,initial_pos_batches = loadDataSocial('./SocialData/social_zara2_test_256_0_50.pickle',set_name=\"test\")\n",
    "\n",
    "    for i, (traj, mask, initial_pos) in enumerate(zip(trajectory_batches,mask_batches,initial_pos_batches)):\n",
    "   \n",
    "        traj -= traj[:, :1, :]\n",
    "        traj *= data_scale\n",
    "        x = traj[:, :past_length,:]\n",
    "        y = traj[:, past_length:,:]\n",
    "\n",
    "        x = x.reshape(-1, x.shape[1]*x.shape[2])\n",
    "\n",
    "        dest = y[:, -1, :]\n",
    "    \n",
    "        destination_errors = []\n",
    "        dectination_recs = []\n",
    "    \n",
    "        for _ in range(best_of_n):\n",
    "            x=tf.constant(x,dtype=tf.float32)\n",
    "            initial_pos=tf.constant(initial_pos,dtype=tf.float32)\n",
    "            dest_rec = model.forward(x, initial_pos)\n",
    "            dectination_recs.append(np.array(dest_rec))\n",
    "\n",
    "            error = np.linalg.norm(dest_rec - dest, axis = 1)\n",
    "            destination_errors.append(error)\n",
    "\n",
    "        destination_errors = np.array(destination_errors)\n",
    "        dectination_recs = np.array(dectination_recs)\n",
    "        # average error\n",
    "        avg_dest_error = np.mean(destination_errors)\n",
    "\n",
    "        indices = np.argmin(destination_errors, axis = 0)\n",
    "\n",
    "        best_dest = dectination_recs[indices,np.arange(x.shape[0]),  :]\n",
    "\n",
    "        # taking the minimum error out of all guess\n",
    "        dest_error = np.mean(np.min(destination_errors, axis = 0))\n",
    "\n",
    "        future_dest = model.predict(x, best_dest, mask, initial_pos)\n",
    "        # final overall prediction\n",
    "        predicted_future = np.concatenate((future_dest, best_dest), axis = 1)\n",
    "        predicted_future = np.reshape(predicted_future, (-1, future_length, 2))\n",
    "        # ADE error\n",
    "        overall_error = np.mean(np.linalg.norm(y - predicted_future, axis = 2))\n",
    "\n",
    "        overall_error /= data_scale\n",
    "        dest_error /= data_scale\n",
    "        avg_dest_error /= data_scale\n",
    "        #print('Test time error in destination best: {:0.3f} and mean: {:0.3f}'.format(dest_error, avg_dest_error))\n",
    "        #print('Test time error overall (ADE) best: {:0.3f}'.format(overall_error))\n",
    "\n",
    "    return overall_error, dest_error, avg_dest_error\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    epochs = 1000\n",
    "    batchSize=100\n",
    "    model=MainModel()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    N=20\n",
    "    best_test_loss = 50 # start saving after this threshold\n",
    "    best_endpoint_loss = 50\n",
    "    for epo in range(epochs):\n",
    "        train_loss, rcl, kld, adl = train(model,optimizer)\n",
    "        test_loss, final_point_loss_best, final_point_loss_avg = test(model, best_of_n = N)\n",
    "                \n",
    "        if best_test_loss > test_loss:\n",
    "            print(\"Epoch: \", epo+1)\n",
    "            print('################## BEST PERFORMANCE {:0.2f} ########'.format(test_loss))\n",
    "            best_test_loss = test_loss\n",
    "        \n",
    "\n",
    "        if final_point_loss_best < best_endpoint_loss:\n",
    "            best_endpoint_loss = final_point_loss_best\n",
    "\n",
    "        print(\"Train Loss\", train_loss)\n",
    "        print(\"RCL\", rcl)\n",
    "        print(\"KLD\", kld)\n",
    "        print(\"ADL\", adl)\n",
    "        print(\"Test ADE\", test_loss)\n",
    "        print(\"Test Average FDE (Across  all samples)\", final_point_loss_avg)\n",
    "        print(\"Test Min FDE\", final_point_loss_best)\n",
    "        print(\"Test Best ADE Loss So Far (N = {})\".format(N), best_test_loss)\n",
    "        print(\"Test Best Min FDE (N = {})\".format(N), best_endpoint_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSocial(load_name,set_name=\"train\", id=False):\n",
    "    \n",
    "    with open(load_name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    traj, masks = data\n",
    "    traj_new = []\n",
    "\n",
    "    if id==False:\n",
    "        for t in traj:\n",
    "            t = np.array(t)\n",
    "            # For SDD\n",
    "            #t = t[:,:,2:]\n",
    "            # For Eth\n",
    "            t = t[:,:,2:4]\n",
    "            traj_new.append(t)\n",
    "            if set_name==\"train\":\n",
    "            #augment training set with reversed tracklets...\n",
    "                reverse_t = np.flip(t, axis=1).copy()\n",
    "                traj_new.append(reverse_t)\n",
    "    else:\n",
    "        for t in traj:\n",
    "            t = np.array(t)\n",
    "            traj_new.append(t)\n",
    "            if set_name==\"train\":\n",
    "                #augment training set with reversed tracklets...\n",
    "                reverse_t = np.flip(t, axis=1).copy()\n",
    "                traj_new.append(reverse_t)\n",
    "    masks_new = []\n",
    "    \n",
    "    for m in masks:\n",
    "        masks_new.append(m)\n",
    "        if set_name==\"train\":\n",
    "            #add second time for the reversed tracklets...\n",
    "            masks_new.append(m)\n",
    "\n",
    "    traj_new = np.array(traj_new)\n",
    "    masks_new = np.array(masks_new)\n",
    "    trajectory_batches = traj_new.copy()\n",
    "    mask_batches = masks_new.copy()\n",
    "    \n",
    "    initial_pos_batches = np.array(initial_pos(trajectory_batches)) # for relative positioning\n",
    "    return trajectory_batches,mask_batches,initial_pos_batches\n",
    "\n",
    "def initial_pos(traj_batches):\n",
    "    batches = []\n",
    "    for b in traj_batches:\n",
    "        starting_pos = b[:,7,:].copy()/1000 #starting pos is end of past, start of future. scaled down.\n",
    "        batches.append(starting_pos)\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-cd8d391ff93e>:36: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  traj_new = np.array(traj_new)\n",
      "<ipython-input-23-cd8d391ff93e>:37: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  masks_new = np.array(masks_new)\n",
      "<ipython-input-23-cd8d391ff93e>:41: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  initial_pos_batches = np.array(initial_pos(trajectory_batches)) # for relative positioning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "################## BEST PERFORMANCE 1.00 ########\n",
      "Train Loss tf.Tensor(8813.178, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(2945.584, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(2155.0444, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(3712.55, shape=(), dtype=float32)\n",
      "Test ADE 0.9964308827796349\n",
      "Test Average FDE (Across  all samples) 1.4849902481161137\n",
      "Test Min FDE 1.2430143612687305\n",
      "Test Best ADE Loss So Far (N = 20) 0.9964308827796349\n",
      "Test Best Min FDE (N = 20) 1.2430143612687305\n",
      "Epoch:  2\n",
      "################## BEST PERFORMANCE 0.75 ########\n",
      "Train Loss tf.Tensor(1433.4117, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(833.8864, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(110.1133, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(489.41202, shape=(), dtype=float32)\n",
      "Test ADE 0.7497046639953902\n",
      "Test Average FDE (Across  all samples) 1.4416299840455413\n",
      "Test Min FDE 1.2376448159576743\n",
      "Test Best ADE Loss So Far (N = 20) 0.7497046639953902\n",
      "Test Best Min FDE (N = 20) 1.2376448159576743\n",
      "Epoch:  3\n",
      "################## BEST PERFORMANCE 0.69 ########\n",
      "Train Loss tf.Tensor(1052.4102, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(781.0749, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(18.688255, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(252.6469, shape=(), dtype=float32)\n",
      "Test ADE 0.6861600835638914\n",
      "Test Average FDE (Across  all samples) 1.432591099892893\n",
      "Test Min FDE 1.2415778252386278\n",
      "Test Best ADE Loss So Far (N = 20) 0.6861600835638914\n",
      "Test Best Min FDE (N = 20) 1.2376448159576743\n",
      "Epoch:  4\n",
      "################## BEST PERFORMANCE 0.69 ########\n",
      "Train Loss tf.Tensor(967.1274, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(740.8541, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(7.4535484, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(218.81982, shape=(), dtype=float32)\n",
      "Test ADE 0.6859155089622109\n",
      "Test Average FDE (Across  all samples) 1.3869790620701286\n",
      "Test Min FDE 1.2021849232335244\n",
      "Test Best ADE Loss So Far (N = 20) 0.6859155089622109\n",
      "Test Best Min FDE (N = 20) 1.2021849232335244\n",
      "Epoch:  5\n",
      "################## BEST PERFORMANCE 0.68 ########\n",
      "Train Loss tf.Tensor(905.6429, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(699.5828, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(4.5752707, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(201.48476, shape=(), dtype=float32)\n",
      "Test ADE 0.6772398524400278\n",
      "Test Average FDE (Across  all samples) 1.3133170784160655\n",
      "Test Min FDE 1.1336882909138997\n",
      "Test Best ADE Loss So Far (N = 20) 0.6772398524400278\n",
      "Test Best Min FDE (N = 20) 1.1336882909138997\n",
      "Epoch:  6\n",
      "################## BEST PERFORMANCE 0.65 ########\n",
      "Train Loss tf.Tensor(871.92566, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(675.3019, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(3.033444, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(193.5906, shape=(), dtype=float32)\n",
      "Test ADE 0.6532847618192088\n",
      "Test Average FDE (Across  all samples) 1.250075909399217\n",
      "Test Min FDE 1.0689863594629432\n",
      "Test Best ADE Loss So Far (N = 20) 0.6532847618192088\n",
      "Test Best Min FDE (N = 20) 1.0689863594629432\n",
      "Epoch:  7\n",
      "################## BEST PERFORMANCE 0.64 ########\n",
      "Train Loss tf.Tensor(846.86694, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(657.8281, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(2.1002262, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(186.9385, shape=(), dtype=float32)\n",
      "Test ADE 0.6391783728629898\n",
      "Test Average FDE (Across  all samples) 1.220261537900535\n",
      "Test Min FDE 1.0542619612909132\n",
      "Test Best ADE Loss So Far (N = 20) 0.6391783728629898\n",
      "Test Best Min FDE (N = 20) 1.0542619612909132\n",
      "Epoch:  8\n",
      "################## BEST PERFORMANCE 0.62 ########\n",
      "Train Loss tf.Tensor(834.1172, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(647.49146, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(1.6295611, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(184.99611, shape=(), dtype=float32)\n",
      "Test ADE 0.6247484766555077\n",
      "Test Average FDE (Across  all samples) 1.1947458790194603\n",
      "Test Min FDE 1.0262050936298985\n",
      "Test Best ADE Loss So Far (N = 20) 0.6247484766555077\n",
      "Test Best Min FDE (N = 20) 1.0262050936298985\n",
      "Epoch:  9\n",
      "################## BEST PERFORMANCE 0.62 ########\n",
      "Train Loss tf.Tensor(819.1112, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(637.73175, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(1.2362821, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(180.1432, shape=(), dtype=float32)\n",
      "Test ADE 0.6177353912488536\n",
      "Test Average FDE (Across  all samples) 1.1871431463508195\n",
      "Test Min FDE 1.0339522874483498\n",
      "Test Best ADE Loss So Far (N = 20) 0.6177353912488536\n",
      "Test Best Min FDE (N = 20) 1.0262050936298985\n",
      "Epoch:  10\n",
      "################## BEST PERFORMANCE 0.62 ########\n",
      "Train Loss tf.Tensor(803.2264, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(627.47076, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(0.9840889, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(174.77141, shape=(), dtype=float32)\n",
      "Test ADE 0.6159121916778416\n",
      "Test Average FDE (Across  all samples) 1.1857227612567203\n",
      "Test Min FDE 1.0410007610115952\n",
      "Test Best ADE Loss So Far (N = 20) 0.6159121916778416\n",
      "Test Best Min FDE (N = 20) 1.0262050936298985\n",
      "Train Loss tf.Tensor(790.7468, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(619.7112, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(0.82453567, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(170.21129, shape=(), dtype=float32)\n",
      "Test ADE 0.617029646290657\n",
      "Test Average FDE (Across  all samples) 1.1919711225776262\n",
      "Test Min FDE 1.045648641483758\n",
      "Test Best ADE Loss So Far (N = 20) 0.6159121916778416\n",
      "Test Best Min FDE (N = 20) 1.0262050936298985\n",
      "Train Loss tf.Tensor(781.1994, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(613.42926, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(0.6986291, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(167.07115, shape=(), dtype=float32)\n",
      "Test ADE 0.6163394849009967\n",
      "Test Average FDE (Across  all samples) 1.1910723101708196\n",
      "Test Min FDE 1.0465394425135786\n",
      "Test Best ADE Loss So Far (N = 20) 0.6159121916778416\n",
      "Test Best Min FDE (N = 20) 1.0262050936298985\n",
      "Epoch:  13\n",
      "################## BEST PERFORMANCE 0.62 ########\n",
      "Train Loss tf.Tensor(774.72064, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(609.4142, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(0.59900296, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(164.70763, shape=(), dtype=float32)\n",
      "Test ADE 0.6158304258146858\n",
      "Test Average FDE (Across  all samples) 1.1808414613046954\n",
      "Test Min FDE 1.0345626261926466\n",
      "Test Best ADE Loss So Far (N = 20) 0.6158304258146858\n",
      "Test Best Min FDE (N = 20) 1.0262050936298985\n",
      "Epoch:  14\n",
      "################## BEST PERFORMANCE 0.61 ########\n",
      "Train Loss tf.Tensor(770.4799, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(607.0051, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(0.50769037, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(162.9672, shape=(), dtype=float32)\n",
      "Test ADE 0.6066553428930307\n",
      "Test Average FDE (Across  all samples) 1.1654247519790484\n",
      "Test Min FDE 1.0277248197986233\n",
      "Test Best ADE Loss So Far (N = 20) 0.6066553428930307\n",
      "Test Best Min FDE (N = 20) 1.0262050936298985\n",
      "Epoch:  15\n",
      "################## BEST PERFORMANCE 0.60 ########\n",
      "Train Loss tf.Tensor(763.8178, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(602.20575, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(0.42826846, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(161.18373, shape=(), dtype=float32)\n",
      "Test ADE 0.596402440458274\n",
      "Test Average FDE (Across  all samples) 1.1380014881010978\n",
      "Test Min FDE 0.9949691833988312\n",
      "Test Best ADE Loss So Far (N = 20) 0.596402440458274\n",
      "Test Best Min FDE (N = 20) 0.9949691833988312\n",
      "Epoch:  16\n",
      "################## BEST PERFORMANCE 0.59 ########\n",
      "Train Loss tf.Tensor(754.3628, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(595.2548, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(0.36800364, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(158.74005, shape=(), dtype=float32)\n",
      "Test ADE 0.5860219576893234\n",
      "Test Average FDE (Across  all samples) 1.116548430535101\n",
      "Test Min FDE 0.972348900251491\n",
      "Test Best ADE Loss So Far (N = 20) 0.5860219576893234\n",
      "Test Best Min FDE (N = 20) 0.972348900251491\n",
      "Epoch:  17\n",
      "################## BEST PERFORMANCE 0.58 ########\n",
      "Train Loss tf.Tensor(751.3151, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(593.1339, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(0.3301053, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(157.85121, shape=(), dtype=float32)\n",
      "Test ADE 0.5819969563438019\n",
      "Test Average FDE (Across  all samples) 1.1088396913261822\n",
      "Test Min FDE 0.9666465943859469\n",
      "Test Best ADE Loss So Far (N = 20) 0.5819969563438019\n",
      "Test Best Min FDE (N = 20) 0.9666465943859469\n",
      "Epoch:  18\n",
      "################## BEST PERFORMANCE 0.57 ########\n",
      "Train Loss tf.Tensor(747.8993, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(590.8931, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(0.29577124, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(156.71028, shape=(), dtype=float32)\n",
      "Test ADE 0.5746543974903062\n",
      "Test Average FDE (Across  all samples) 1.0874448284026115\n",
      "Test Min FDE 0.9575245200946766\n",
      "Test Best ADE Loss So Far (N = 20) 0.5746543974903062\n",
      "Test Best Min FDE (N = 20) 0.9575245200946766\n"
     ]
    }
   ],
   "source": [
    "run_train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
