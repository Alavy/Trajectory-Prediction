{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params\n",
    "kld_reg= 1\n",
    "adl_reg=1\n",
    "\n",
    "fdim=16\n",
    "zdim=16\n",
    "sigma=1.3\n",
    "past_length=8\n",
    "future_length=12\n",
    "data_scale=170\n",
    "enc_past_size=(past_length*2,512,256,fdim)\n",
    "enc_dest_size=(2,8,16,fdim)\n",
    "enc_latent_size=(2*fdim,8,50,2*zdim)\n",
    "dec_size=(fdim + zdim,1024,512,1024,2)\n",
    "predictor_size=(2*fdim,1024,512,256,2*(future_length-1))\n",
    "learning_rate=0.0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(file_path: str):\n",
    "  npz = np.load(file_path, allow_pickle=True)\n",
    "  return npz['observations'], npz['obs_speed'], npz['targets'], npz[\n",
    "      'target_speed'], npz['mean'], npz['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(tf.Module):\n",
    "  def __init__(self, input_dim, output_size, name=None):\n",
    "    super(Dense, self).__init__(name=name)\n",
    "    self.w = tf.Variable(tf.random.uniform([input_dim, output_size],-(1.0/input_dim)**0.5,(1.0/input_dim)**0.5 ),name='w',dtype=tf.float32)\n",
    "    self.b = tf.Variable(tf.random.uniform([output_size],-(1.0/input_dim)**0.5,(1.0/input_dim)**0.5 ), name='b',dtype=tf.float32)\n",
    "  def __call__(self, x):\n",
    "    x = tf.constant(x,dtype=tf.float32)\n",
    "    y = tf.matmul(x, self.w) + self.b\n",
    "    return tf.nn.relu(y)\n",
    "\n",
    "class FullyConnectedNeuralNet(tf.Module):\n",
    "  def __init__(self,sizes, name=None):\n",
    "    super(FullyConnectedNeuralNet, self).__init__(name=name)\n",
    "    self.layers = []\n",
    "    with self.name_scope:\n",
    "      for i in range(len(sizes)-1):\n",
    "        self.layers.append(Dense(input_dim=sizes[i], output_size=sizes[i+1]))\n",
    "  @tf.Module.with_name_scope\n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainModel(tf.Module):\n",
    "  def __init__(self,name=None):\n",
    "    super(MainModel, self).__init__(name=name)\n",
    "\n",
    "    self.zdim = zdim\n",
    "    self.sigma = sigma\n",
    "\n",
    "    self.pastEncoder = FullyConnectedNeuralNet(enc_past_size)\n",
    "\n",
    "    self.destEncoder = FullyConnectedNeuralNet(enc_dest_size)\n",
    "\n",
    "    self.latentDistributionEncoder = FullyConnectedNeuralNet(enc_latent_size)\n",
    "\n",
    "    self.latentDistributionDecoder = FullyConnectedNeuralNet(dec_size)\n",
    "\n",
    "    self.predictorNetwork = FullyConnectedNeuralNet(predictor_size)\n",
    "\n",
    "  def forward(self, x, dest = []):\n",
    "\n",
    "    if len(dest):\n",
    "        self.training=True\n",
    "    else:\n",
    "        self.training=False\n",
    "        \n",
    "    # encode\n",
    "    traj_past_ftr = self.pastEncoder(x)\n",
    "    #print(f\"ftraj max {ftraj.numpy().max()}\")\n",
    "    if not self.training:\n",
    "        z = tf.random.normal((x.shape[0], self.zdim),0,self.sigma)\n",
    "\n",
    "    else:\n",
    "        dest_ftr = self.destEncoder(dest)\n",
    "        #print(f\"dest_features Max {dest_features.numpy().max()}\")\n",
    "\n",
    "        concat_ftr = tf.concat((traj_past_ftr, dest_ftr), axis = 1)\n",
    "        latent =  self.latentDistributionEncoder(concat_ftr)\n",
    "        mu = latent[:, 0:self.zdim] # 2-d array\n",
    "        logvar = latent[:, self.zdim:] # 2-d array\n",
    "\n",
    "        var = tf.math.exp(logvar*0.5)\n",
    "        #print(f\"var {var}\")\n",
    "        eps = tf.random.normal(var.shape)\n",
    "        z = eps*var + mu\n",
    "        #print(f\"z -> {z}\")\n",
    "\n",
    "\n",
    "    latentDistributionDecoder_input = tf.concat((traj_past_ftr, z), axis = 1)\n",
    "    generated_dest = self.latentDistributionDecoder(latentDistributionDecoder_input)\n",
    "    \n",
    "    if self.training:\n",
    "        generated_dest_ftr = self.destEncoder(generated_dest)\n",
    "        prediction_ftr = tf.concat((traj_past_ftr, generated_dest_ftr), axis = 1)\n",
    "        pred_future = self.predictorNetwork(prediction_ftr)\n",
    "        \n",
    "        return (generated_dest, mu, logvar, pred_future)\n",
    "    else:\n",
    "        return generated_dest\n",
    "\n",
    "  def predict(self, past, generated_dest):\n",
    "        \n",
    "    traj_past_ftr = self.pastEncoder(past)\n",
    "    generated_dest_ftr = self.destEncoder(generated_dest)\n",
    "    prediction_ftr = tf.concat((traj_past_ftr, generated_dest_ftr), axis = 1)\n",
    "    future_traj = self.predictorNetwork(prediction_ftr)\n",
    "    return future_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(dest, dest_rec, mean, log_var, future, future_rec):\n",
    "    \n",
    "    rcl = tf.math.reduce_mean(tf.keras.metrics.mean_squared_error(dest, dest_rec))\n",
    "    adl = tf.math.reduce_mean(tf.keras.metrics.mean_squared_error(future, future_rec))\n",
    "\n",
    "    kld = -0.5 * tf.math.reduce_sum(1 + log_var - mean**2 - tf.math.exp(log_var))\n",
    "\n",
    "    return rcl, kld, adl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(X,batchSize):\n",
    "    start = random.randint(0, len(X)-batchSize)\n",
    "    return X[start:start+batchSize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trajx,model,optimizer):\n",
    "    train_loss = 0\n",
    "    total_rcl, total_kld, total_adl = 0, 0, 0\n",
    "    \n",
    "    traj = trajx - trajx[:, :1, :]\n",
    "    traj *= data_scale\n",
    "\n",
    "    x = traj[:, :past_length, :]\n",
    "    y = traj[:, past_length:, :]\n",
    "\n",
    "    x = x.reshape(-1, x.shape[1]*x.shape[2]) # (x,y,x,y ... )\n",
    "    dest = y[:, -1, :]\n",
    "    future = y[:, :-1, :].reshape(y.shape[0],-1)\n",
    "                \n",
    "    #x.astype(np.float64)\n",
    "    #print(f\"dest-> {trajx.shape}\")\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        x=tf.constant(x,dtype=tf.float32)\n",
    "        tape.watch(x)\n",
    "        dest_rec, mu, var, future_rec = model.forward(x, dest=dest)\n",
    "        #print(f\"dest_recon {dest_recon}\")\n",
    "        #print(f\"mu {mu}\")\n",
    "        #print(f\"var {var}\")\n",
    "        #print(f\"interpolated_future {interpolated_future}\")\n",
    "                    \n",
    "        rcl, kld, adl = calculate_loss(dest, dest_rec, mu, var, future, future_rec)\n",
    "                    \n",
    "        loss = rcl + kld * kld_reg + adl * adl_reg\n",
    "        grad_sub = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grad_sub, model.trainable_variables))\n",
    "                    \n",
    "        #print(f\"total Loss {loss}\")\n",
    "        #print(f\"rcl Loss {rcl}\")\n",
    "        #print(f\"kld Loss {kld}\")\n",
    "        #print(f\"adl Loss {adl}\")\n",
    "    train_loss+=loss\n",
    "    total_rcl+=rcl\n",
    "    total_kld+=kld\n",
    "    total_adl+=adl\n",
    "    return train_loss, total_rcl, total_kld, total_adl\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(trajx, model, best_of_n = 1):\n",
    "    \n",
    "    traj = trajx - trajx[:, :1, :]\n",
    "    traj *= data_scale\n",
    "\n",
    "    x = traj[:, :past_length, :]\n",
    "    y = traj[:, past_length:, :]\n",
    "\n",
    "    x = x.reshape(-1, x.shape[1]*x.shape[2])\n",
    "\n",
    "    dest = y[:, -1, :]\n",
    "    \n",
    "    destination_errors = []\n",
    "    dectination_recs = []\n",
    "    \n",
    "    for _ in range(best_of_n):\n",
    "        x=tf.constant(x,dtype=tf.float32)\n",
    "        dest_rec = model.forward(x)\n",
    "        dectination_recs.append(np.array(dest_rec))\n",
    "\n",
    "        error = np.linalg.norm(dest_rec - dest, axis = 1)\n",
    "        destination_errors.append(error)\n",
    "\n",
    "    destination_errors = np.array(destination_errors)\n",
    "    dectination_recs = np.array(dectination_recs)\n",
    "    # average error\n",
    "    avg_dest_error = np.mean(destination_errors)\n",
    "\n",
    "    indices = np.argmin(destination_errors, axis = 0)\n",
    "\n",
    "    best_dest = dectination_recs[indices,np.arange(x.shape[0]),  :]\n",
    "\n",
    "    # taking the minimum error out of all guess\n",
    "    dest_error = np.mean(np.min(destination_errors, axis = 0))\n",
    "\n",
    "    future_dest = model.predict(x, best_dest)\n",
    "    # final overall prediction\n",
    "    predicted_future = np.concatenate((future_dest, best_dest), axis = 1)\n",
    "    predicted_future = np.reshape(predicted_future, (-1, future_length, 2))\n",
    "    # ADE error\n",
    "    overall_error = np.mean(np.linalg.norm(y - predicted_future, axis = 2))\n",
    "\n",
    "    overall_error /= data_scale\n",
    "    dest_error /= data_scale\n",
    "    avg_dest_error /= data_scale\n",
    "    #print('Test time error in destination best: {:0.3f} and mean: {:0.3f}'.format(dest_error, avg_dest_error))\n",
    "    #print('Test time error overall (ADE) best: {:0.3f}'.format(overall_error))\n",
    "\n",
    "    return overall_error, dest_error, avg_dest_error\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    observations, _, targets, _, _, _ = loadData('./data/eth/eth_train.npz')\n",
    "    train_dataset = np.concatenate([observations, targets], axis=1)\n",
    "    observations, _, targets, _, _, _ = loadData('./data/eth/eth_test.npz')\n",
    "    test_dataset = np.concatenate([observations, targets], axis=1)\n",
    "    \n",
    "    epochs = 8\n",
    "    batchSize=100\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model=MainModel()\n",
    "    N=20\n",
    "    best_test_loss = 50 # start saving after this threshold\n",
    "    best_endpoint_loss = 50\n",
    "    for epo in range(epochs):\n",
    "            #print(f\"Epoch : {epo+1}\")\n",
    "            for it in range(int(len(train_dataset)/batchSize)):\n",
    "                trajx_train = next_batch(train_dataset,batchSize)\n",
    "                trajx_test = next_batch(test_dataset,len(test_dataset))\n",
    "                \n",
    "                train_loss, rcl, kld, adl = train(trajx_train, model,optimizer)\n",
    "                test_loss, final_point_loss_best, final_point_loss_avg = test(trajx_test, model, best_of_n = N)\n",
    "                \n",
    "                if best_test_loss > test_loss:\n",
    "                    print(\"Epoch: \", epo+1)\n",
    "                    print('################## BEST PERFORMANCE {:0.2f} ########'.format(test_loss))\n",
    "                    best_test_loss = test_loss\n",
    "                \"\"\"\"\n",
    "                if best_test_loss < 10.25:\n",
    "                    save_path = './content/trained.pt'\n",
    "                \"\"\"\n",
    "\n",
    "                if final_point_loss_best < best_endpoint_loss:\n",
    "                    best_endpoint_loss = final_point_loss_best\n",
    "\n",
    "                print(\"Train Loss\", train_loss)\n",
    "                print(\"RCL\", rcl)\n",
    "                print(\"KLD\", kld)\n",
    "                print(\"ADL\", adl)\n",
    "                print(\"Test ADE\", test_loss)\n",
    "                print(\"Test Average FDE (Across  all samples)\", final_point_loss_avg)\n",
    "                print(\"Test Min FDE\", final_point_loss_best)\n",
    "                print(\"Test Best ADE Loss So Far (N = {})\".format(N), best_test_loss)\n",
    "                print(\"Test Best Min FDE (N = {})\".format(N), best_endpoint_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dest-> (30307, 20, 2)\n",
      "Epoch:  1\n",
      "################## BEST PERFORMANCE 4.98 ########\n",
      "Train Loss tf.Tensor(127799520.0, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(422538.7, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(127160770.0, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(216217.94, shape=(), dtype=float32)\n",
      "Test ADE 4.983596108225233\n",
      "Test Average FDE (Across  all samples) 6.510464298023897\n",
      "Test Min FDE 6.510459271599265\n",
      "Test Best ADE Loss So Far (N = 20) 4.983596108225233\n",
      "Test Best Min FDE (N = 20) 6.510459271599265\n",
      "dest-> (30307, 20, 2)\n",
      "Epoch:  1\n",
      "################## BEST PERFORMANCE 4.98 ########\n",
      "Train Loss tf.Tensor(16692468.0, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(422502.12, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(16053777.0, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(216189.02, shape=(), dtype=float32)\n",
      "Test ADE 4.982743053827522\n",
      "Test Average FDE (Across  all samples) 6.510466452205883\n",
      "Test Min FDE 6.510454963235294\n",
      "Test Best ADE Loss So Far (N = 20) 4.982743053827522\n",
      "Test Best Min FDE (N = 20) 6.510454963235294\n",
      "dest-> (30307, 20, 2)\n",
      "Epoch:  1\n",
      "################## BEST PERFORMANCE 4.98 ########\n",
      "Train Loss tf.Tensor(8038782.0, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(422502.0, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(7400128.5, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(216151.55, shape=(), dtype=float32)\n",
      "Test ADE 4.981090210474643\n",
      "Test Average FDE (Across  all samples) 6.508965705422794\n",
      "Test Min FDE 6.508661965762868\n",
      "Test Best ADE Loss So Far (N = 20) 4.981090210474643\n",
      "Test Best Min FDE (N = 20) 6.508661965762868\n",
      "dest-> (30307, 20, 2)\n",
      "Epoch:  1\n",
      "################## BEST PERFORMANCE 4.98 ########\n",
      "Train Loss tf.Tensor(4802226.5, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(422397.6, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(4163750.2, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(216078.66, shape=(), dtype=float32)\n",
      "Test ADE 4.978313899613092\n",
      "Test Average FDE (Across  all samples) 6.501584041819853\n",
      "Test Min FDE 6.501209214154412\n",
      "Test Best ADE Loss So Far (N = 20) 4.978313899613092\n",
      "Test Best Min FDE (N = 20) 6.501209214154412\n",
      "dest-> (30307, 20, 2)\n",
      "Epoch:  1\n",
      "################## BEST PERFORMANCE 4.97 ########\n",
      "Train Loss tf.Tensor(3360721.0, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(421974.78, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(2722778.2, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(215967.9, shape=(), dtype=float32)\n",
      "Test ADE 4.974531494062012\n",
      "Test Average FDE (Across  all samples) 6.490438304227941\n",
      "Test Min FDE 6.489997414981618\n",
      "Test Best ADE Loss So Far (N = 20) 4.974531494062012\n",
      "Test Best Min FDE (N = 20) 6.489997414981618\n",
      "dest-> (30307, 20, 2)\n",
      "Epoch:  1\n",
      "################## BEST PERFORMANCE 4.97 ########\n",
      "Train Loss tf.Tensor(2665232.5, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(421269.38, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(2028143.5, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(215819.61, shape=(), dtype=float32)\n",
      "Test ADE 4.969712909995436\n",
      "Test Average FDE (Across  all samples) 6.475689338235294\n",
      "Test Min FDE 6.475193876378676\n",
      "Test Best ADE Loss So Far (N = 20) 4.969712909995436\n",
      "Test Best Min FDE (N = 20) 6.475193876378676\n",
      "dest-> (30307, 20, 2)\n",
      "Epoch:  1\n",
      "################## BEST PERFORMANCE 4.96 ########\n",
      "Train Loss tf.Tensor(2236745.8, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(420346.8, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(1600762.4, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(215636.44, shape=(), dtype=float32)\n",
      "Test ADE 4.963694435525552\n",
      "Test Average FDE (Across  all samples) 6.456479779411764\n",
      "Test Min FDE 6.4559469784007355\n",
      "Test Best ADE Loss So Far (N = 20) 4.963694435525552\n",
      "Test Best Min FDE (N = 20) 6.4559469784007355\n",
      "dest-> (30307, 20, 2)\n",
      "Epoch:  1\n",
      "################## BEST PERFORMANCE 4.96 ########\n",
      "Train Loss tf.Tensor(1930428.0, shape=(), dtype=float32)\n",
      "RCL tf.Tensor(419159.38, shape=(), dtype=float32)\n",
      "KLD tf.Tensor(1295856.5, shape=(), dtype=float32)\n",
      "ADL tf.Tensor(215412.08, shape=(), dtype=float32)\n",
      "Test ADE 4.956241727491124\n",
      "Test Average FDE (Across  all samples) 6.431263643152573\n",
      "Test Min FDE 6.430681295955883\n",
      "Test Best ADE Loss So Far (N = 20) 4.956241727491124\n",
      "Test Best Min FDE (N = 20) 6.430681295955883\n",
      "dest-> (30307, 20, 2)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-68274d7e071c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-5bb4e41b3899>\u001b[0m in \u001b[0;36mrun_train\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mtrajx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                 \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkld\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m                 \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_point_loss_best\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_point_loss_avg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrajx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_of_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-4a71355300d9>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(trajx, model, optimizer)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mdest_rec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfuture_rec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;31m#print(f\"dest_recon {dest_recon}\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m#print(f\"mu {mu}\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-ff072e87d625>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, dest)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# encode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mtraj_past_ftr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpastEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[1;31m#print(f\"ftraj max {ftraj.numpy().max()}\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\AnacondaFiles\\lib\\site-packages\\tensorflow\\python\\module\\module.py\u001b[0m in \u001b[0;36mmethod_with_name_scope\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmethod_with_name_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod_with_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-89abe66a2094>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-89abe66a2094>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mFullyConnectedNeuralNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\AnacondaFiles\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(features, name)\u001b[0m\n\u001b[0;32m  10521\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10522\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10523\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m  10524\u001b[0m         _ctx, \"Relu\", name, features)\n\u001b[0;32m  10525\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
